{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, you've learned about visualizing PySpark DataFrame properties, data cleaning using PySpark, and how to create new features.\n",
    "\n",
    "Now it's time to apply this knowledge to a new dataset.\n",
    "\n",
    "In this notebook, you'll work with the products dataset that we've downloaded together with the orders dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load the dataset into a spark dataframe and display it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_products = (\n",
    "    spark.read.format('csv')\n",
    "    .option('inferSchema', 'true')\n",
    "    .option('header', 'true')\n",
    "    .option('sep', ',')\n",
    "    .load('/FileStore/lp-big-data/orders-data/product-supplier.csv')\n",
    ")\n",
    "\n",
    "df_products.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Print the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_products.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. How many observations does the dataset contain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_products.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Rename the columns to standardize them.\n",
    "\n",
    "- Lower case all column names\n",
    "- Replace the spaces with underscores `_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_products_renamed = (\n",
    "    df_products\n",
    "    .toDF('product_id',\n",
    "        'product_line',\n",
    "        'product_category',\n",
    "        'product_group',\n",
    "        'product_name',\n",
    "        'supplier_country',\n",
    "        'supplier_name',\n",
    "        'supplier_id',\n",
    "        'announcement_date',\n",
    "        'launch_date',\n",
    "    )\n",
    ")\n",
    "\n",
    "df_products_renamed.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Check if there are missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_products_renamed.describe().display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the missing categories by 'Unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_products_filled = (\n",
    "    df_products_renamed\n",
    "    .fillna('Unknown', subset=(['product_category']))\n",
    ")\n",
    "\n",
    "df_products_filled.describe().display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. What are the names of all possible product lines?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_products_filled.groupBy('product_line').count().display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Create a new column called `lead_time` with the amount of time in days between each product's announcement and launch dates\n",
    "\n",
    "Hint: Check out the [DateTime functions](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html#datetime-functions) from the PySpark SQL library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_products_lead = (\n",
    "    df_products_filled\n",
    "    .withColumn(\n",
    "        'lead_time',\n",
    "        f.datediff(f.col('launch_date'), f.col('announcement_date'))\n",
    "    )\n",
    ")\n",
    "\n",
    "df_products_lead.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. The announcement and launch dates are in UTC format. Convert them to New York's timezone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_products_timezone = (\n",
    "    df_products_lead\n",
    "    .withColumn('announcement_date', f.from_utc_timestamp(f.col('announcement_date'), 'America/New_York'))\n",
    "    .withColumn('launch_date', f.from_utc_timestamp(f.col('launch_date'), 'America/New_York'))\n",
    ")\n",
    "\n",
    "df_products_timezone.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Create a new column with the suppliers' continent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_products_timezone.groupBy('supplier_country').count().display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_products_preprocessed = (\n",
    "    df_products_timezone\n",
    "    .withColumn(\n",
    "        'supplier_continent',\n",
    "        f.when(\n",
    "            f.col('supplier_country').isin(\n",
    "                ['NL', 'PT', 'GB', 'DE', 'ES', 'FR', 'NO', 'DK', 'BE', 'SE']\n",
    "                ),\n",
    "            'Europe'\n",
    "        )\n",
    "        .when(f.col('supplier_country').isin(['CA', 'US']), 'America')\n",
    "        .when(f.col('supplier_country').isin(['AU']), 'Oceania')\n",
    "        .otherwise(f.lit('Unkown'))\n",
    "    )\n",
    ")\n",
    "\n",
    "df_products_preprocessed.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Save the data to the dbfs to the folder `/FileStore/lp-big-data/preprocessed-data/orders-data/` under the name `/products-preprocessed.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_products_preprocessed.write.csv('/FileStore/lp-big-data/preprocessed-data/orders-data/products-preprocessed.csv', header=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
